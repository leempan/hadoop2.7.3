2017-02-09 21:24:45,048 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = leempans-macbook-pro.local/192.168.1.11
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /Users/leempan/LEEMPAN/hadoop-2.7.3/etc/hadoop:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
2017-02-09 21:24:45,062 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-02-09 21:24:45,067 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-02-09 21:24:45,347 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-02-09 21:24:45,426 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-02-09 21:24:45,426 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-02-09 21:24:45,429 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://ubuntu1:9000
2017-02-09 21:24:45,430 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use ubuntu1:9000 to access this namenode/service.
2017-02-09 21:24:47,551 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-02-09 21:24:47,690 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-02-09 21:24:47,757 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-02-09 21:24:47,764 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-02-09 21:24:47,770 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-02-09 21:24:47,776 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-02-09 21:24:47,777 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-02-09 21:24:47,777 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-02-09 21:24:47,778 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-02-09 21:24:47,902 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-02-09 21:24:47,905 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-02-09 21:24:47,934 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-02-09 21:24:47,934 INFO org.mortbay.log: jetty-6.1.26
2017-02-09 21:24:48,152 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-02-09 21:24:48,184 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 21:24:48,184 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 21:24:48,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-02-09 21:24:48,214 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-02-09 21:24:48,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-02-09 21:24:48,258 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-02-09 21:24:48,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-02-09 21:24:48,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Feb 09 21:24:48
2017-02-09 21:24:48,260 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-02-09 21:24:48,260 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 21:24:48,262 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-02-09 21:24:48,262 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-02-09 21:24:48,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-02-09 21:24:48,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2017-02-09 21:24:48,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-02-09 21:24:48,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-02-09 21:24:48,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-02-09 21:24:48,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-02-09 21:24:48,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-02-09 21:24:48,267 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-02-09 21:24:48,274 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = leempan (auth:SIMPLE)
2017-02-09 21:24:48,274 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-02-09 21:24:48,275 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-02-09 21:24:48,275 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-02-09 21:24:48,276 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-02-09 21:24:48,419 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-02-09 21:24:48,419 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 21:24:48,420 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-02-09 21:24:48,420 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-02-09 21:24:48,420 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-02-09 21:24:48,420 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-02-09 21:24:48,420 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-02-09 21:24:48,420 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-02-09 21:24:48,426 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-02-09 21:24:48,426 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 21:24:48,426 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-02-09 21:24:48,427 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-02-09 21:24:48,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-02-09 21:24:48,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-02-09 21:24:48,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-02-09 21:24:48,431 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-02-09 21:24:48,431 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-02-09 21:24:48,431 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-02-09 21:24:48,432 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-02-09 21:24:48,433 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-02-09 21:24:48,434 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-02-09 21:24:48,434 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 21:24:48,435 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-02-09 21:24:48,435 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-02-09 21:24:48,451 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/in_use.lock acquired by nodename 59597@leempans-macbook-pro.local
2017-02-09 21:24:48,503 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current
2017-02-09 21:24:48,504 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2017-02-09 21:24:48,504 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-02-09 21:24:48,553 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-02-09 21:24:48,607 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-02-09 21:24:48,607 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000000
2017-02-09 21:24:48,720 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-02-09 21:24:48,721 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2017-02-09 21:24:48,862 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-02-09 21:24:48,862 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 425 msecs
2017-02-09 21:24:49,095 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to ubuntu1:9000
2017-02-09 21:24:49,103 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-02-09 21:24:49,121 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-02-09 21:24:49,159 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-02-09 21:24:49,201 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 21:24:49,201 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 21:24:49,201 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-02-09 21:24:49,201 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2017-02-09 21:24:49,201 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-02-09 21:24:49,201 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-02-09 21:24:49,210 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 21:24:49,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-02-09 21:24:49,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-02-09 21:24:49,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-02-09 21:24:49,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-02-09 21:24:49,213 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-02-09 21:24:49,213 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec
2017-02-09 21:24:49,241 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-02-09 21:24:49,242 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-02-09 21:24:49,244 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: ubuntu1/192.168.1.11:9000
2017-02-09 21:24:49,244 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-02-09 21:24:49,248 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-02-09 21:24:52,802 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0) storage a08915f1-3c69-4b2c-ba27-0185b3d2a8f9
2017-02-09 21:24:52,804 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 21:24:52,806 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.1.11:50010
2017-02-09 21:24:52,929 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 21:24:52,929 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-49b10856-865b-4636-a955-077090905b46 for DN 192.168.1.11:50010
2017-02-09 21:24:52,988 INFO BlockStateChange: BLOCK* processReport: from storage DS-49b10856-865b-4636-a955-077090905b46 node DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2017-02-09 21:24:57,955 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.1.11
2017-02-09 21:24:57,956 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-02-09 21:24:57,956 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 1
2017-02-09 21:24:57,956 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 5 
2017-02-09 21:24:57,958 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000001 -> /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000001-0000000000000000002
2017-02-09 21:24:57,960 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 3
2017-02-09 21:24:59,114 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-02-09 21:24:59,115 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000002 size 354 bytes.
2017-02-09 21:24:59,117 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2017-02-09 21:30:59,091 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-02-09 21:30:59,093 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at leempans-macbook-pro.local/192.168.1.11
************************************************************/
2017-02-09 21:32:11,205 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = leempans-macbook-pro.local/192.168.1.11
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /Users/leempan/LEEMPAN/hadoop-2.7.3/etc/hadoop:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/Users/leempan/LEEMPAN/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
2017-02-09 21:32:11,214 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-02-09 21:32:11,219 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-02-09 21:32:11,493 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-02-09 21:32:11,577 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-02-09 21:32:11,577 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-02-09 21:32:11,579 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://ubuntu1:9000
2017-02-09 21:32:11,580 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use ubuntu1:9000 to access this namenode/service.
2017-02-09 21:32:11,668 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-02-09 21:32:11,786 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-02-09 21:32:11,842 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-02-09 21:32:11,848 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-02-09 21:32:11,853 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-02-09 21:32:11,857 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-02-09 21:32:11,859 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-02-09 21:32:11,859 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-02-09 21:32:11,859 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-02-09 21:32:11,982 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-02-09 21:32:11,983 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-02-09 21:32:12,007 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-02-09 21:32:12,007 INFO org.mortbay.log: jetty-6.1.26
2017-02-09 21:32:12,211 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-02-09 21:32:12,246 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 21:32:12,246 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 21:32:12,282 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-02-09 21:32:12,282 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-02-09 21:32:12,321 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-02-09 21:32:12,321 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-02-09 21:32:12,322 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-02-09 21:32:12,323 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Feb 09 21:32:12
2017-02-09 21:32:12,324 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-02-09 21:32:12,324 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 21:32:12,325 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-02-09 21:32:12,325 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-02-09 21:32:12,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-02-09 21:32:12,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2017-02-09 21:32:12,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-02-09 21:32:12,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-02-09 21:32:12,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-02-09 21:32:12,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-02-09 21:32:12,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-02-09 21:32:12,331 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-02-09 21:32:12,338 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = leempan (auth:SIMPLE)
2017-02-09 21:32:12,338 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-02-09 21:32:12,338 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-02-09 21:32:12,338 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-02-09 21:32:12,339 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-02-09 21:32:12,474 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-02-09 21:32:12,474 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 21:32:12,474 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-02-09 21:32:12,474 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-02-09 21:32:12,475 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-02-09 21:32:12,475 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-02-09 21:32:12,475 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-02-09 21:32:12,475 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-02-09 21:32:12,481 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-02-09 21:32:12,481 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 21:32:12,481 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-02-09 21:32:12,481 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-02-09 21:32:12,482 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-02-09 21:32:12,482 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-02-09 21:32:12,482 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-02-09 21:32:12,484 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-02-09 21:32:12,484 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-02-09 21:32:12,484 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-02-09 21:32:12,485 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-02-09 21:32:12,485 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-02-09 21:32:12,487 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-02-09 21:32:12,487 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 21:32:12,487 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-02-09 21:32:12,487 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-02-09 21:32:12,502 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/in_use.lock acquired by nodename 60424@leempans-macbook-pro.local
2017-02-09 21:32:12,557 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current
2017-02-09 21:32:12,598 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000003 -> /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000003-0000000000000000003
2017-02-09 21:32:12,604 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2017-02-09 21:32:12,643 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-02-09 21:32:12,745 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-02-09 21:32:12,745 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 2 from /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000002
2017-02-09 21:32:12,746 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@53d1b9b3 expecting start txid #3
2017-02-09 21:32:12,746 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000003-0000000000000000003
2017-02-09 21:32:12,747 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000003-0000000000000000003' to transaction ID 3
2017-02-09 21:32:12,749 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000003-0000000000000000003 of size 1048576 edits # 1 loaded in 0 seconds
2017-02-09 21:32:12,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-02-09 21:32:12,754 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4
2017-02-09 21:32:12,886 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-02-09 21:32:12,886 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 396 msecs
2017-02-09 21:32:13,035 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to ubuntu1:9000
2017-02-09 21:32:13,042 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-02-09 21:32:13,056 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-02-09 21:32:13,084 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-02-09 21:32:13,102 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 21:32:13,103 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 21:32:13,103 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-02-09 21:32:13,104 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2017-02-09 21:32:13,104 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2017-02-09 21:32:13,104 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2017-02-09 21:32:13,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 21:32:13,121 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2017-02-09 21:32:13,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-02-09 21:32:13,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2017-02-09 21:32:13,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-02-09 21:32:13,122 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-02-09 21:32:13,122 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 16 msec
2017-02-09 21:32:13,138 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-02-09 21:32:13,139 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-02-09 21:32:13,141 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: ubuntu1/192.168.1.11:9000
2017-02-09 21:32:13,141 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-02-09 21:32:13,144 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-02-09 21:32:22,102 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0) storage a08915f1-3c69-4b2c-ba27-0185b3d2a8f9
2017-02-09 21:32:22,102 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 21:32:22,102 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.1.11:50010
2017-02-09 21:32:22,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 21:32:22,170 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-49b10856-865b-4636-a955-077090905b46 for DN 192.168.1.11:50010
2017-02-09 21:32:22,205 INFO BlockStateChange: BLOCK* processReport: from storage DS-49b10856-865b-4636-a955-077090905b46 node DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2017-02-09 21:33:30,937 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.1.11
2017-02-09 21:33:30,937 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-02-09 21:33:30,937 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 4
2017-02-09 21:33:30,937 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 20 
2017-02-09 21:33:30,939 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 21 
2017-02-09 21:33:30,941 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000004 -> /Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000004-0000000000000000005
2017-02-09 21:33:30,941 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 6
2017-02-09 21:33:31,851 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-02-09 21:33:31,852 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000005 size 354 bytes.
2017-02-09 21:33:31,855 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 2
2017-02-09 21:33:31,856 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/Users/leempan/LEEMPAN/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2017-02-09 21:35:43,438 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2017-02-09 21:37:00,915 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 3 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 10 
2017-02-09 21:37:00,962 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-49b10856-865b-4636-a955-077090905b46:NORMAL:192.168.1.11:50010|RBW]]} for /input/koubei_segged.txt._COPYING_
2017-02-09 21:37:01,278 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-49b10856-865b-4636-a955-077090905b46:NORMAL:192.168.1.11:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /input/koubei_segged.txt._COPYING_
2017-02-09 21:37:01,291 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 192.168.1.11:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-49b10856-865b-4636-a955-077090905b46:NORMAL:192.168.1.11:50010|RBW]]} size 631104
2017-02-09 21:37:01,690 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/koubei_segged.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_1153282423_1
2017-02-09 21:38:26,153 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 11 Total time for transactions(ms): 16 Number of transactions batched in Syncs: 1 Number of syncs: 7 SyncTimes(ms): 11 
2017-02-09 21:38:27,329 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-49b10856-865b-4636-a955-077090905b46:NORMAL:192.168.1.11:50010|RBW]]} for /output1/_temporary/0/_temporary/attempt_local1762635820_0001_r_000000_0/part-r-00000
2017-02-09 21:38:27,561 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 192.168.1.11:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-49b10856-865b-4636-a955-077090905b46:NORMAL:192.168.1.11:50010|RBW]]} size 0
2017-02-09 21:38:27,566 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output1/_temporary/0/_temporary/attempt_local1762635820_0001_r_000000_0/part-r-00000 is closed by DFSClient_NONMAPREDUCE_77994739_1
2017-02-09 21:38:27,623 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /output1/_SUCCESS is closed by DFSClient_NONMAPREDUCE_77994739_1
2017-02-09 22:19:51,289 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-02-09 22:19:51,291 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at leempans-macbook-pro.local/192.168.1.11
************************************************************/
2017-02-09 22:42:54,595 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = leempans-macbook-pro.local/192.168.1.11
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop-2.7.3/etc/hadoop:/opt/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
2017-02-09 22:42:54,606 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-02-09 22:42:54,612 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-02-09 22:42:54,888 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-02-09 22:42:54,974 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-02-09 22:42:54,974 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-02-09 22:42:54,977 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://ubuntu1:9000
2017-02-09 22:42:54,978 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use ubuntu1:9000 to access this namenode/service.
2017-02-09 22:42:55,060 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-02-09 22:42:55,186 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-02-09 22:42:55,234 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-02-09 22:42:55,249 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-02-09 22:42:55,254 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-02-09 22:42:55,258 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-02-09 22:42:55,260 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-02-09 22:42:55,260 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-02-09 22:42:55,260 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-02-09 22:42:55,375 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-02-09 22:42:55,376 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-02-09 22:42:55,399 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-02-09 22:42:55,400 INFO org.mortbay.log: jetty-6.1.26
2017-02-09 22:42:55,616 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-02-09 22:42:55,656 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 22:42:55,656 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 22:42:55,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-02-09 22:42:55,706 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-02-09 22:42:55,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-02-09 22:42:55,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-02-09 22:42:55,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-02-09 22:42:55,755 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Feb 09 22:42:55
2017-02-09 22:42:55,756 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-02-09 22:42:55,756 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 22:42:55,757 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-02-09 22:42:55,758 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-02-09 22:42:55,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-02-09 22:42:55,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2017-02-09 22:42:55,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-02-09 22:42:55,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-02-09 22:42:55,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-02-09 22:42:55,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-02-09 22:42:55,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-02-09 22:42:55,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-02-09 22:42:55,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = leempan (auth:SIMPLE)
2017-02-09 22:42:55,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-02-09 22:42:55,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-02-09 22:42:55,771 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-02-09 22:42:55,772 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-02-09 22:42:55,927 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-02-09 22:42:55,928 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 22:42:55,928 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-02-09 22:42:55,928 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-02-09 22:42:55,929 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-02-09 22:42:55,929 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-02-09 22:42:55,929 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-02-09 22:42:55,929 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-02-09 22:42:55,938 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-02-09 22:42:55,938 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 22:42:55,938 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-02-09 22:42:55,938 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-02-09 22:42:55,940 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-02-09 22:42:55,940 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-02-09 22:42:55,940 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-02-09 22:42:55,943 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-02-09 22:42:55,944 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-02-09 22:42:55,944 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-02-09 22:42:55,945 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-02-09 22:42:55,945 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-02-09 22:42:55,947 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-02-09 22:42:55,947 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 22:42:55,948 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-02-09 22:42:55,948 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-02-09 22:42:55,966 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop-2.7.3/tmp/dfs/name/in_use.lock acquired by nodename 63634@leempans-macbook-pro.local
2017-02-09 22:42:56,015 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /opt/hadoop-2.7.3/tmp/dfs/name/current
2017-02-09 22:42:56,110 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000006 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000006-0000000000000000028
2017-02-09 22:42:56,192 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000005, cpktTxId=0000000000000000005)
2017-02-09 22:42:56,216 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2017-02-09 22:42:56,236 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-02-09 22:42:56,237 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 5 from /opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000005
2017-02-09 22:42:56,237 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@295eaa7c expecting start txid #6
2017-02-09 22:42:56,237 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000006-0000000000000000028
2017-02-09 22:42:56,238 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000006-0000000000000000028' to transaction ID 6
2017-02-09 22:42:56,263 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000006-0000000000000000028 of size 1048576 edits # 23 loaded in 0 seconds
2017-02-09 22:42:56,264 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2017-02-09 22:42:56,264 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2017-02-09 22:42:56,268 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage.ckpt_0000000000000000028 using no compression
2017-02-09 22:42:56,302 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage.ckpt_0000000000000000028 of size 710 bytes saved in 0 seconds.
2017-02-09 22:42:56,306 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 5
2017-02-09 22:42:56,306 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000002, cpktTxId=0000000000000000002)
2017-02-09 22:42:56,313 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 29
2017-02-09 22:42:56,428 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-02-09 22:42:56,428 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 477 msecs
2017-02-09 22:42:56,594 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to ubuntu1:9000
2017-02-09 22:42:56,599 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-02-09 22:42:56,609 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-02-09 22:42:56,634 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-02-09 22:42:56,651 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 22:42:56,651 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 22:42:56,651 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-02-09 22:42:56,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 22:42:56,676 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-02-09 22:42:56,677 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-02-09 22:42:56,679 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: ubuntu1/192.168.1.11:9000
2017-02-09 22:42:56,679 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-02-09 22:42:56,683 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-02-09 22:46:05,585 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.NamenodeProtocol.rollEditLog from 192.168.1.11:52994 Call#1 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Log not rolled. Name node is in safe mode.
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-02-09 22:46:20,393 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-02-09 22:46:20,394 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at leempans-macbook-pro.local/192.168.1.11
************************************************************/
2017-02-09 22:55:45,629 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = leempans-macbook-pro.local/192.168.1.11
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop-2.7.3/etc/hadoop:/opt/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
2017-02-09 22:55:45,639 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-02-09 22:55:45,644 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-02-09 22:55:45,920 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-02-09 22:55:46,009 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-02-09 22:55:46,009 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-02-09 22:55:46,012 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://ubuntu1:9000
2017-02-09 22:55:46,013 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use ubuntu1:9000 to access this namenode/service.
2017-02-09 22:55:46,096 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-02-09 22:55:46,218 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-02-09 22:55:46,265 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-02-09 22:55:46,279 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-02-09 22:55:46,283 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-02-09 22:55:46,288 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-02-09 22:55:46,290 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-02-09 22:55:46,290 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-02-09 22:55:46,290 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-02-09 22:55:46,401 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-02-09 22:55:46,402 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-02-09 22:55:46,424 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-02-09 22:55:46,424 INFO org.mortbay.log: jetty-6.1.26
2017-02-09 22:55:46,628 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-02-09 22:55:46,665 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 22:55:46,666 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 22:55:46,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-02-09 22:55:46,695 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-02-09 22:55:46,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-02-09 22:55:46,735 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-02-09 22:55:46,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-02-09 22:55:46,736 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Feb 09 22:55:46
2017-02-09 22:55:46,738 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-02-09 22:55:46,738 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 22:55:46,739 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-02-09 22:55:46,739 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-02-09 22:55:46,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-02-09 22:55:46,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2017-02-09 22:55:46,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-02-09 22:55:46,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-02-09 22:55:46,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-02-09 22:55:46,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-02-09 22:55:46,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-02-09 22:55:46,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-02-09 22:55:46,751 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = leempan (auth:SIMPLE)
2017-02-09 22:55:46,751 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-02-09 22:55:46,751 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-02-09 22:55:46,752 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-02-09 22:55:46,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-02-09 22:55:46,908 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-02-09 22:55:46,908 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 22:55:46,908 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-02-09 22:55:46,908 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-02-09 22:55:46,909 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-02-09 22:55:46,909 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-02-09 22:55:46,909 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-02-09 22:55:46,909 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-02-09 22:55:46,915 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-02-09 22:55:46,915 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 22:55:46,915 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-02-09 22:55:46,915 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-02-09 22:55:46,916 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-02-09 22:55:46,916 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-02-09 22:55:46,916 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-02-09 22:55:46,919 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-02-09 22:55:46,919 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-02-09 22:55:46,919 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-02-09 22:55:46,920 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-02-09 22:55:46,920 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-02-09 22:55:46,921 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-02-09 22:55:46,921 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 22:55:46,922 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-02-09 22:55:46,922 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-02-09 22:55:46,935 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop-2.7.3/tmp/dfs/name/in_use.lock acquired by nodename 64868@leempans-macbook-pro.local
2017-02-09 22:55:46,977 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /opt/hadoop-2.7.3/tmp/dfs/name/current
2017-02-09 22:55:47,030 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000029 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000029-0000000000000000029
2017-02-09 22:55:47,036 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000028, cpktTxId=0000000000000000028)
2017-02-09 22:55:47,082 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 6 INodes.
2017-02-09 22:55:47,205 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-02-09 22:55:47,206 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 28 from /opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000028
2017-02-09 22:55:47,206 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4504d271 expecting start txid #29
2017-02-09 22:55:47,206 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000029-0000000000000000029
2017-02-09 22:55:47,208 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000029-0000000000000000029' to transaction ID 29
2017-02-09 22:55:47,210 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000029-0000000000000000029 of size 1048576 edits # 1 loaded in 0 seconds
2017-02-09 22:55:47,215 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-02-09 22:55:47,215 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 30
2017-02-09 22:55:47,348 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-02-09 22:55:47,348 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 423 msecs
2017-02-09 22:55:47,498 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to ubuntu1:9000
2017-02-09 22:55:47,507 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-02-09 22:55:47,521 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-02-09 22:55:47,544 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-02-09 22:55:47,559 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 22:55:47,559 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 22:55:47,559 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-02-09 22:55:47,564 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 22:55:47,583 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-02-09 22:55:47,583 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-02-09 22:55:47,586 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: ubuntu1/192.168.1.11:9000
2017-02-09 22:55:47,586 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-02-09 22:55:47,590 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-02-09 23:01:27,501 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-02-09 23:01:27,504 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at leempans-macbook-pro.local/192.168.1.11
************************************************************/
2017-02-09 23:06:53,811 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = leempans-macbook-pro.local/192.168.1.11
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop-2.7.3/etc/hadoop:/opt/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
2017-02-09 23:06:53,820 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-02-09 23:06:53,828 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-02-09 23:06:54,110 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-02-09 23:06:54,189 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-02-09 23:06:54,189 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-02-09 23:06:54,192 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://ubuntu1:9000
2017-02-09 23:06:54,193 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use ubuntu1:9000 to access this namenode/service.
2017-02-09 23:06:54,277 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-02-09 23:06:54,400 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-02-09 23:06:54,448 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-02-09 23:06:54,461 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-02-09 23:06:54,466 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-02-09 23:06:54,471 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-02-09 23:06:54,472 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-02-09 23:06:54,473 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-02-09 23:06:54,473 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-02-09 23:06:54,591 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-02-09 23:06:54,592 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-02-09 23:06:54,613 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-02-09 23:06:54,613 INFO org.mortbay.log: jetty-6.1.26
2017-02-09 23:06:54,854 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-02-09 23:06:54,885 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:06:54,885 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:06:54,915 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-02-09 23:06:54,915 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-02-09 23:06:54,946 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-02-09 23:06:54,946 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-02-09 23:06:54,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-02-09 23:06:54,947 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Feb 09 23:06:54
2017-02-09 23:06:54,949 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-02-09 23:06:54,955 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:06:54,957 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-02-09 23:06:54,957 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-02-09 23:06:54,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-02-09 23:06:54,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2017-02-09 23:06:54,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-02-09 23:06:54,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-02-09 23:06:54,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-02-09 23:06:54,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-02-09 23:06:54,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-02-09 23:06:54,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-02-09 23:06:54,969 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = leempan (auth:SIMPLE)
2017-02-09 23:06:54,970 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-02-09 23:06:54,970 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-02-09 23:06:54,970 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-02-09 23:06:54,971 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-02-09 23:06:55,109 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-02-09 23:06:55,109 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:06:55,110 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-02-09 23:06:55,110 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-02-09 23:06:55,110 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-02-09 23:06:55,110 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-02-09 23:06:55,110 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-02-09 23:06:55,110 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-02-09 23:06:55,116 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-02-09 23:06:55,116 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:06:55,116 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-02-09 23:06:55,116 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-02-09 23:06:55,118 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-02-09 23:06:55,118 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-02-09 23:06:55,118 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-02-09 23:06:55,121 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-02-09 23:06:55,121 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-02-09 23:06:55,121 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-02-09 23:06:55,122 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-02-09 23:06:55,123 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-02-09 23:06:55,124 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-02-09 23:06:55,124 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:06:55,124 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-02-09 23:06:55,124 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-02-09 23:06:55,137 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop-2.7.3/tmp/dfs/name/in_use.lock acquired by nodename 66045@leempans-macbook-pro.local
2017-02-09 23:06:55,183 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /opt/hadoop-2.7.3/tmp/dfs/name/current
2017-02-09 23:06:55,215 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000030 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000030-0000000000000000030
2017-02-09 23:06:55,225 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000028, cpktTxId=0000000000000000028)
2017-02-09 23:06:55,300 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 6 INodes.
2017-02-09 23:06:55,421 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-02-09 23:06:55,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 28 from /opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000028
2017-02-09 23:06:55,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@163d04ff expecting start txid #29
2017-02-09 23:06:55,422 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000029-0000000000000000029
2017-02-09 23:06:55,423 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000029-0000000000000000029' to transaction ID 29
2017-02-09 23:06:55,431 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000029-0000000000000000029 of size 1048576 edits # 1 loaded in 0 seconds
2017-02-09 23:06:55,431 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7c209437 expecting start txid #30
2017-02-09 23:06:55,431 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000030-0000000000000000030
2017-02-09 23:06:55,431 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000030-0000000000000000030' to transaction ID 29
2017-02-09 23:06:55,432 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000030-0000000000000000030 of size 1048576 edits # 1 loaded in 0 seconds
2017-02-09 23:06:55,436 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-02-09 23:06:55,437 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 31
2017-02-09 23:06:55,553 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-02-09 23:06:55,554 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 426 msecs
2017-02-09 23:06:55,712 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to ubuntu1:9000
2017-02-09 23:06:55,721 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-02-09 23:06:55,736 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-02-09 23:06:55,765 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-02-09 23:06:55,780 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:06:55,780 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:06:55,780 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-02-09 23:06:55,785 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:06:55,805 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-02-09 23:06:55,805 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-02-09 23:06:55,807 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: ubuntu1/192.168.1.11:9000
2017-02-09 23:06:55,807 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-02-09 23:06:55,810 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-02-09 23:07:00,123 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0) storage a08915f1-3c69-4b2c-ba27-0185b3d2a8f9
2017-02-09 23:07:00,123 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:07:00,124 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.1.11:50010
2017-02-09 23:07:00,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:07:00,220 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-49b10856-865b-4636-a955-077090905b46 for DN 192.168.1.11:50010
2017-02-09 23:07:00,263 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 1 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2017-02-09 23:07:00,263 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-02-09 23:07:00,265 INFO BlockStateChange: BLOCK* processReport: from storage DS-49b10856-865b-4636-a955-077090905b46 node DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0), blocks: 2, hasStaleStorage: false, processing time: 9 msecs
2017-02-09 23:07:00,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 2
2017-02-09 23:07:00,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-02-09 23:07:00,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2017-02-09 23:07:00,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-02-09 23:07:00,268 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-02-09 23:07:00,268 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2017-02-09 23:07:20,327 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 2 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-02-09 23:07:30,355 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2017-02-09 23:07:30,356 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-02-09 23:07:30,356 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-02-09 23:07:30,356 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 2 blocks
2017-02-09 23:08:15,358 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.1.11
2017-02-09 23:08:15,358 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-02-09 23:08:15,358 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 31
2017-02-09 23:08:15,359 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 5 
2017-02-09 23:08:15,360 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 5 
2017-02-09 23:08:15,362 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000031 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000031-0000000000000000032
2017-02-09 23:08:15,362 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 33
2017-02-09 23:08:16,309 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-02-09 23:08:16,310 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000032 size 710 bytes.
2017-02-09 23:08:16,312 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 28
2017-02-09 23:08:16,312 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000005, cpktTxId=0000000000000000005)
2017-02-09 23:11:14,947 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-02-09 23:11:14,949 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at leempans-macbook-pro.local/192.168.1.11
************************************************************/
2017-02-09 23:14:23,385 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = leempans-macbook-pro.local/192.168.1.11
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop-2.7.3/etc/hadoop:/opt/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
2017-02-09 23:14:23,396 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-02-09 23:14:23,401 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-02-09 23:14:23,677 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-02-09 23:14:23,759 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-02-09 23:14:23,759 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-02-09 23:14:23,761 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://ubuntu1:9000
2017-02-09 23:14:23,762 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use ubuntu1:9000 to access this namenode/service.
2017-02-09 23:14:23,841 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-02-09 23:14:23,964 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-02-09 23:14:24,017 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-02-09 23:14:24,032 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-02-09 23:14:24,037 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-02-09 23:14:24,041 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-02-09 23:14:24,043 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-02-09 23:14:24,043 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-02-09 23:14:24,043 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-02-09 23:14:24,164 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-02-09 23:14:24,165 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-02-09 23:14:24,187 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-02-09 23:14:24,188 INFO org.mortbay.log: jetty-6.1.26
2017-02-09 23:14:24,408 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-02-09 23:14:24,432 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:14:24,432 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:14:24,461 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-02-09 23:14:24,461 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-02-09 23:14:24,498 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-02-09 23:14:24,498 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-02-09 23:14:24,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-02-09 23:14:24,499 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Feb 09 23:14:24
2017-02-09 23:14:24,508 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-02-09 23:14:24,508 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:14:24,509 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-02-09 23:14:24,509 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-02-09 23:14:24,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-02-09 23:14:24,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2017-02-09 23:14:24,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-02-09 23:14:24,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-02-09 23:14:24,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-02-09 23:14:24,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-02-09 23:14:24,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-02-09 23:14:24,516 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-02-09 23:14:24,523 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = leempan (auth:SIMPLE)
2017-02-09 23:14:24,523 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-02-09 23:14:24,523 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-02-09 23:14:24,523 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-02-09 23:14:24,524 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-02-09 23:14:24,664 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-02-09 23:14:24,664 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:14:24,665 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-02-09 23:14:24,665 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-02-09 23:14:24,665 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-02-09 23:14:24,665 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-02-09 23:14:24,665 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-02-09 23:14:24,665 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-02-09 23:14:24,671 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-02-09 23:14:24,671 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:14:24,672 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-02-09 23:14:24,672 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-02-09 23:14:24,673 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-02-09 23:14:24,673 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-02-09 23:14:24,673 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-02-09 23:14:24,675 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-02-09 23:14:24,675 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-02-09 23:14:24,675 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-02-09 23:14:24,676 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-02-09 23:14:24,676 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-02-09 23:14:24,678 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-02-09 23:14:24,678 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:14:24,678 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-02-09 23:14:24,678 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-02-09 23:14:24,693 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop-2.7.3/tmp/dfs/name/in_use.lock acquired by nodename 67507@leempans-macbook-pro.local
2017-02-09 23:14:24,746 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /opt/hadoop-2.7.3/tmp/dfs/name/current
2017-02-09 23:14:24,788 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000033 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000033-0000000000000000033
2017-02-09 23:14:24,793 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000032, cpktTxId=0000000000000000032)
2017-02-09 23:14:24,842 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 6 INodes.
2017-02-09 23:14:24,963 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-02-09 23:14:24,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 32 from /opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000032
2017-02-09 23:14:24,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4504d271 expecting start txid #33
2017-02-09 23:14:24,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000033-0000000000000000033
2017-02-09 23:14:24,965 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000033-0000000000000000033' to transaction ID 33
2017-02-09 23:14:24,967 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000033-0000000000000000033 of size 1048576 edits # 1 loaded in 0 seconds
2017-02-09 23:14:24,972 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-02-09 23:14:24,973 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 34
2017-02-09 23:14:25,101 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-02-09 23:14:25,101 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 420 msecs
2017-02-09 23:14:25,269 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to ubuntu1:9000
2017-02-09 23:14:25,276 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-02-09 23:14:25,289 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-02-09 23:14:25,317 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-02-09 23:14:25,333 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:14:25,333 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:14:25,333 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-02-09 23:14:25,338 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:14:25,357 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-02-09 23:14:25,357 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-02-09 23:14:25,359 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: ubuntu1/192.168.1.11:9000
2017-02-09 23:14:25,360 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-02-09 23:14:25,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-02-09 23:14:29,433 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:14:29,433 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#0 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:14:29,734 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0) storage a08915f1-3c69-4b2c-ba27-0185b3d2a8f9
2017-02-09 23:14:29,734 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:14:29,735 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.1.11:50010
2017-02-09 23:14:29,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:14:29,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-49b10856-865b-4636-a955-077090905b46 for DN 192.168.1.11:50010
2017-02-09 23:14:29,860 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 1 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 30 seconds.
2017-02-09 23:14:29,860 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-02-09 23:14:29,861 INFO BlockStateChange: BLOCK* processReport: from storage DS-49b10856-865b-4636-a955-077090905b46 node DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0), blocks: 2, hasStaleStorage: false, processing time: 8 msecs
2017-02-09 23:14:29,864 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 2
2017-02-09 23:14:29,864 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-02-09 23:14:29,864 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2017-02-09 23:14:29,864 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-02-09 23:14:29,864 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-02-09 23:14:29,864 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 3 msec
2017-02-09 23:14:34,447 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#1 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:14:39,527 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#2 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:14:44,537 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#3 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:14:49,657 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#4 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:14:49,915 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 2 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-02-09 23:14:54,767 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#5 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:14:59,911 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:14:59,912 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#6 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:14:59,941 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2017-02-09 23:14:59,941 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-02-09 23:14:59,941 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-02-09 23:14:59,941 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 2 blocks
2017-02-09 23:15:05,001 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#7 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:10,123 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#8 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:15,222 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#9 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:20,340 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#10 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:25,385 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#11 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:30,414 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:15:30,414 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#12 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:35,475 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#13 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:36,804 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.1.11
2017-02-09 23:15:36,804 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-02-09 23:15:36,804 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 34
2017-02-09 23:15:36,804 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 4 
2017-02-09 23:15:36,804 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 4 
2017-02-09 23:15:36,806 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000034 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000034-0000000000000000035
2017-02-09 23:15:36,806 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 36
2017-02-09 23:15:37,476 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-02-09 23:15:37,476 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000035 size 710 bytes.
2017-02-09 23:15:37,479 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 32
2017-02-09 23:15:37,479 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000028, cpktTxId=0000000000000000028)
2017-02-09 23:15:40,567 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#14 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:45,574 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#15 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:50,578 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#16 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:15:55,582 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#17 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:00,608 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:16:00,608 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#18 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:05,612 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#19 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:10,617 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#20 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:15,621 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#21 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:20,625 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#22 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:25,629 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#23 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:30,652 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:16:30,653 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#24 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:35,656 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#25 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:40,661 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#26 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:45,665 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#27 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:50,669 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#28 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:16:55,675 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#29 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:00,696 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:17:00,696 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#30 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:05,701 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#31 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:10,723 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#32 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:15,905 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#33 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:20,909 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#34 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:26,027 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#35 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:31,053 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:17:31,053 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#36 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:36,154 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#37 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:41,158 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#38 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:46,279 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#39 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:51,282 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#40 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:17:56,295 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#41 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:01,408 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:18:01,408 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#42 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:06,436 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#43 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:11,570 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#44 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:16,575 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#45 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:21,578 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#46 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:26,648 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#47 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:31,721 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:18:31,721 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#48 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:36,725 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#49 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:41,821 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#50 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:46,975 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#51 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:52,145 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#52 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:18:57,150 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#53 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:19:02,177 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:19:02,177 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47676 Call#54 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:19:03,865 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-02-09 23:19:03,866 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at leempans-macbook-pro.local/192.168.1.11
************************************************************/
2017-02-09 23:19:53,612 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = leempans-macbook-pro.local/192.168.1.11
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop-2.7.3/etc/hadoop:/opt/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
2017-02-09 23:19:53,623 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-02-09 23:19:53,627 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-02-09 23:19:53,917 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-02-09 23:19:54,003 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-02-09 23:19:54,003 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-02-09 23:19:54,005 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://ubuntu1:9000
2017-02-09 23:19:54,006 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use ubuntu1:9000 to access this namenode/service.
2017-02-09 23:19:54,106 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-02-09 23:19:54,229 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-02-09 23:19:54,277 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-02-09 23:19:54,292 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-02-09 23:19:54,297 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-02-09 23:19:54,301 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-02-09 23:19:54,303 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-02-09 23:19:54,303 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-02-09 23:19:54,303 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-02-09 23:19:54,417 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-02-09 23:19:54,419 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-02-09 23:19:54,443 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-02-09 23:19:54,443 INFO org.mortbay.log: jetty-6.1.26
2017-02-09 23:19:54,653 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-02-09 23:19:54,683 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:19:54,683 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:19:54,713 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-02-09 23:19:54,713 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-02-09 23:19:54,747 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-02-09 23:19:54,747 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-02-09 23:19:54,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-02-09 23:19:54,748 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Feb 09 23:19:54
2017-02-09 23:19:54,756 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-02-09 23:19:54,756 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:19:54,757 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-02-09 23:19:54,757 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-02-09 23:19:54,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-02-09 23:19:54,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2017-02-09 23:19:54,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-02-09 23:19:54,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-02-09 23:19:54,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-02-09 23:19:54,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-02-09 23:19:54,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-02-09 23:19:54,763 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-02-09 23:19:54,769 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = leempan (auth:SIMPLE)
2017-02-09 23:19:54,769 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-02-09 23:19:54,769 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-02-09 23:19:54,769 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-02-09 23:19:54,770 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-02-09 23:19:54,909 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-02-09 23:19:54,909 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:19:54,909 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-02-09 23:19:54,909 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-02-09 23:19:54,910 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-02-09 23:19:54,910 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-02-09 23:19:54,910 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-02-09 23:19:54,911 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-02-09 23:19:54,917 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-02-09 23:19:54,917 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:19:54,917 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-02-09 23:19:54,917 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-02-09 23:19:54,918 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-02-09 23:19:54,918 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-02-09 23:19:54,918 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-02-09 23:19:54,920 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-02-09 23:19:54,920 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-02-09 23:19:54,920 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-02-09 23:19:54,921 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-02-09 23:19:54,922 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-02-09 23:19:54,923 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-02-09 23:19:54,923 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:19:54,923 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-02-09 23:19:54,923 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-02-09 23:19:54,937 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop-2.7.3/tmp/dfs/name/in_use.lock acquired by nodename 69510@leempans-macbook-pro.local
2017-02-09 23:19:54,986 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /opt/hadoop-2.7.3/tmp/dfs/name/current
2017-02-09 23:19:55,032 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000036 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000036-0000000000000000036
2017-02-09 23:19:55,041 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000035, cpktTxId=0000000000000000035)
2017-02-09 23:19:55,075 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 6 INodes.
2017-02-09 23:19:55,197 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-02-09 23:19:55,197 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 35 from /opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000035
2017-02-09 23:19:55,197 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4504d271 expecting start txid #36
2017-02-09 23:19:55,198 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000036-0000000000000000036
2017-02-09 23:19:55,200 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000036-0000000000000000036' to transaction ID 36
2017-02-09 23:19:55,202 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000036-0000000000000000036 of size 1048576 edits # 1 loaded in 0 seconds
2017-02-09 23:19:55,208 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-02-09 23:19:55,209 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 37
2017-02-09 23:19:55,348 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-02-09 23:19:55,348 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 422 msecs
2017-02-09 23:19:55,485 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to ubuntu1:9000
2017-02-09 23:19:55,491 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-02-09 23:19:55,504 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-02-09 23:19:55,534 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-02-09 23:19:55,549 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:19:55,549 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:19:55,549 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-02-09 23:19:55,555 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:19:55,576 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-02-09 23:19:55,576 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-02-09 23:19:55,579 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: ubuntu1/192.168.1.11:9000
2017-02-09 23:19:55,579 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-02-09 23:19:55,582 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-02-09 23:19:59,473 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:19:59,475 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#0 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:01,826 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0) storage a08915f1-3c69-4b2c-ba27-0185b3d2a8f9
2017-02-09 23:20:01,826 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:20:01,827 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.1.11:50010
2017-02-09 23:20:01,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:20:01,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-49b10856-865b-4636-a955-077090905b46 for DN 192.168.1.11:50010
2017-02-09 23:20:01,967 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 1 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2017-02-09 23:20:01,967 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-02-09 23:20:01,969 INFO BlockStateChange: BLOCK* processReport: from storage DS-49b10856-865b-4636-a955-077090905b46 node DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0), blocks: 2, hasStaleStorage: false, processing time: 10 msecs
2017-02-09 23:20:01,972 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 2
2017-02-09 23:20:01,972 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-02-09 23:20:01,972 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2017-02-09 23:20:01,972 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-02-09 23:20:01,972 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-02-09 23:20:01,972 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2017-02-09 23:20:04,556 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#1 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:09,667 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#2 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:14,796 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#3 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:19,800 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#4 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:22,025 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 2 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-02-09 23:20:24,804 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#5 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:29,833 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:20:29,833 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#6 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:32,053 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 37 secs
2017-02-09 23:20:32,054 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-02-09 23:20:32,054 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-02-09 23:20:32,054 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 2 blocks
2017-02-09 23:20:34,837 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#7 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:39,844 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#8 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:44,848 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#9 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:49,966 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#10 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:20:55,081 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#11 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:00,109 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:21:00,109 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#12 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:05,202 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#13 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:05,213 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.1.11
2017-02-09 23:21:05,213 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-02-09 23:21:05,213 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 37
2017-02-09 23:21:05,213 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 30 
2017-02-09 23:21:05,215 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 30 
2017-02-09 23:21:05,216 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000037 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000037-0000000000000000038
2017-02-09 23:21:05,216 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 39
2017-02-09 23:21:05,899 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-02-09 23:21:05,899 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000038 size 710 bytes.
2017-02-09 23:21:05,902 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 35
2017-02-09 23:21:05,902 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000032, cpktTxId=0000000000000000032)
2017-02-09 23:21:10,318 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#14 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:15,322 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#15 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:20,485 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#16 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:25,489 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#17 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:30,530 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:21:30,530 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#18 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:35,682 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#19 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:40,690 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#20 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:45,694 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#21 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:50,743 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#22 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:21:55,757 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#23 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:00,790 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:22:00,790 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#24 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:05,796 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#25 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:10,800 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#26 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:15,804 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#27 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:20,811 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#28 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:25,816 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#29 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:30,840 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:22:30,840 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#30 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:35,846 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#31 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:40,851 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#32 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:45,857 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#33 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:50,861 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#34 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:22:55,866 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#35 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:00,893 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:23:00,893 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#36 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:05,897 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#37 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:10,904 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#38 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:15,914 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#39 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:20,923 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#40 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:25,931 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#41 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:30,952 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:23:30,953 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#42 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:35,957 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#43 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:40,962 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#44 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:45,968 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#45 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:50,972 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#46 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:23:55,975 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#47 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:00,998 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:24:00,998 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#48 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:06,015 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#49 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:11,019 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#50 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:16,023 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#51 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:21,027 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#52 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:26,033 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#53 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:31,056 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:24:31,056 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#54 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:36,066 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#55 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:41,072 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#56 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:46,081 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#57 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:51,085 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#58 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:24:56,089 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#59 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:25:01,118 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:25:01,118 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#60 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:25:06,162 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#61 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:25:11,276 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#62 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:25:16,279 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47723 Call#63 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:25:20,186 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-02-09 23:25:20,188 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at leempans-macbook-pro.local/192.168.1.11
************************************************************/
2017-02-09 23:26:27,500 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = leempans-macbook-pro.local/192.168.1.11
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop-2.7.3/etc/hadoop:/opt/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
2017-02-09 23:26:27,511 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-02-09 23:26:27,516 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-02-09 23:26:27,803 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-02-09 23:26:27,883 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-02-09 23:26:27,883 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-02-09 23:26:27,885 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://ubuntu1:9000
2017-02-09 23:26:27,886 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use ubuntu1:9000 to access this namenode/service.
2017-02-09 23:26:27,968 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-02-09 23:26:28,110 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-02-09 23:26:28,156 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-02-09 23:26:28,170 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-02-09 23:26:28,176 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-02-09 23:26:28,180 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-02-09 23:26:28,182 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-02-09 23:26:28,183 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-02-09 23:26:28,183 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-02-09 23:26:28,303 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-02-09 23:26:28,304 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-02-09 23:26:28,328 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-02-09 23:26:28,328 INFO org.mortbay.log: jetty-6.1.26
2017-02-09 23:26:28,539 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-02-09 23:26:28,569 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:26:28,569 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:26:28,598 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-02-09 23:26:28,598 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-02-09 23:26:28,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-02-09 23:26:28,637 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-02-09 23:26:28,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-02-09 23:26:28,638 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Feb 09 23:26:28
2017-02-09 23:26:28,640 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-02-09 23:26:28,640 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:26:28,641 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-02-09 23:26:28,641 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-02-09 23:26:28,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-02-09 23:26:28,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2017-02-09 23:26:28,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-02-09 23:26:28,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-02-09 23:26:28,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-02-09 23:26:28,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-02-09 23:26:28,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-02-09 23:26:28,648 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-02-09 23:26:28,656 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = leempan (auth:SIMPLE)
2017-02-09 23:26:28,656 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-02-09 23:26:28,656 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-02-09 23:26:28,656 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-02-09 23:26:28,657 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-02-09 23:26:28,804 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-02-09 23:26:28,804 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:26:28,804 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-02-09 23:26:28,804 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-02-09 23:26:28,805 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-02-09 23:26:28,805 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-02-09 23:26:28,805 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-02-09 23:26:28,805 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-02-09 23:26:28,811 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-02-09 23:26:28,811 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:26:28,811 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-02-09 23:26:28,811 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-02-09 23:26:28,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-02-09 23:26:28,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-02-09 23:26:28,812 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-02-09 23:26:28,815 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-02-09 23:26:28,815 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-02-09 23:26:28,815 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-02-09 23:26:28,816 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-02-09 23:26:28,816 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-02-09 23:26:28,818 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-02-09 23:26:28,818 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:26:28,818 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-02-09 23:26:28,818 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-02-09 23:26:28,835 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop-2.7.3/tmp/dfs/name/in_use.lock acquired by nodename 70747@leempans-macbook-pro.local
2017-02-09 23:26:28,888 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /opt/hadoop-2.7.3/tmp/dfs/name/current
2017-02-09 23:26:28,925 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000039 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000039-0000000000000000039
2017-02-09 23:26:28,932 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000038, cpktTxId=0000000000000000038)
2017-02-09 23:26:28,978 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 6 INodes.
2017-02-09 23:26:29,100 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-02-09 23:26:29,100 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 38 from /opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000038
2017-02-09 23:26:29,100 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4504d271 expecting start txid #39
2017-02-09 23:26:29,100 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000039-0000000000000000039
2017-02-09 23:26:29,102 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000039-0000000000000000039' to transaction ID 39
2017-02-09 23:26:29,104 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000039-0000000000000000039 of size 1048576 edits # 1 loaded in 0 seconds
2017-02-09 23:26:29,109 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-02-09 23:26:29,111 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 40
2017-02-09 23:26:29,233 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-02-09 23:26:29,233 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 412 msecs
2017-02-09 23:26:29,378 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to ubuntu1:9000
2017-02-09 23:26:29,385 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-02-09 23:26:29,401 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-02-09 23:26:29,436 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-02-09 23:26:29,457 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:26:29,457 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:26:29,457 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-02-09 23:26:29,463 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:26:29,485 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-02-09 23:26:29,485 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-02-09 23:26:29,487 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: ubuntu1/192.168.1.11:9000
2017-02-09 23:26:29,488 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-02-09 23:26:29,491 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-02-09 23:26:33,487 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:26:33,489 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#0 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:26:34,613 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0) storage a08915f1-3c69-4b2c-ba27-0185b3d2a8f9
2017-02-09 23:26:34,613 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:26:34,614 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.1.11:50010
2017-02-09 23:26:34,701 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:26:34,701 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-49b10856-865b-4636-a955-077090905b46 for DN 192.168.1.11:50010
2017-02-09 23:26:34,742 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 1 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2017-02-09 23:26:34,742 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-02-09 23:26:34,744 INFO BlockStateChange: BLOCK* processReport: from storage DS-49b10856-865b-4636-a955-077090905b46 node DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0), blocks: 2, hasStaleStorage: false, processing time: 7 msecs
2017-02-09 23:26:34,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 2
2017-02-09 23:26:34,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-02-09 23:26:34,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2017-02-09 23:26:34,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-02-09 23:26:34,746 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-02-09 23:26:34,746 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2017-02-09 23:26:38,515 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#1 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:26:43,633 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#2 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:26:48,639 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#3 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:26:53,754 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#4 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:26:54,785 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 2 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-02-09 23:26:58,758 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#5 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:03,794 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:27:03,795 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#6 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:04,807 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 36 secs
2017-02-09 23:27:04,807 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-02-09 23:27:04,808 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-02-09 23:27:04,808 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 2 blocks
2017-02-09 23:27:08,799 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#7 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:13,806 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#8 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:18,810 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#9 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:23,813 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#10 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:28,835 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#11 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:33,856 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:27:33,856 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#12 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:38,860 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#13 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:38,888 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.1.11
2017-02-09 23:27:38,888 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-02-09 23:27:38,888 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 40
2017-02-09 23:27:38,889 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2017-02-09 23:27:38,890 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 9 
2017-02-09 23:27:38,891 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000040 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000040-0000000000000000041
2017-02-09 23:27:38,891 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 42
2017-02-09 23:27:39,599 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-02-09 23:27:39,599 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000041 size 710 bytes.
2017-02-09 23:27:39,601 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 38
2017-02-09 23:27:39,601 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000035, cpktTxId=0000000000000000035)
2017-02-09 23:27:43,867 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#14 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:48,872 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#15 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:53,879 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#16 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:27:58,883 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#17 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:28:03,907 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:28:03,907 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#18 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:28:08,910 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#19 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:28:13,938 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#20 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:28:18,942 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#21 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:28:23,947 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#22 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:28:28,951 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47725 Call#23 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:28:33,986 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-02-09 23:28:33,988 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at leempans-macbook-pro.local/192.168.1.11
************************************************************/
2017-02-09 23:29:52,445 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = leempans-macbook-pro.local/192.168.1.11
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /opt/hadoop-2.7.3/etc/hadoop:/opt/hadoop-2.7.3/share/hadoop/common/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-configuration-1.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-digester-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-httpclient-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/commons-net-3.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-client-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-framework-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/gson-2.2.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpclient-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/httpcore-4.2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jets3t-0.9.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsch-0.1.42.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/mockito-all-1.8.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/common/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/common/hadoop-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/activation-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-cli-1.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-codec-1.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-lang-2.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guava-11.0.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-client-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-json-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jettison-1.1.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/servlet-api-2.5.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/asm-3.2.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/javax.inject-1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/junit-4.11.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/lib/xz-1.0.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_31
************************************************************/
2017-02-09 23:29:52,456 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2017-02-09 23:29:52,461 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2017-02-09 23:29:52,753 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2017-02-09 23:29:52,838 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2017-02-09 23:29:52,838 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2017-02-09 23:29:52,841 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://ubuntu1:9000
2017-02-09 23:29:52,841 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use ubuntu1:9000 to access this namenode/service.
2017-02-09 23:29:54,927 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2017-02-09 23:29:55,051 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2017-02-09 23:29:55,117 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2017-02-09 23:29:55,134 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2017-02-09 23:29:55,139 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2017-02-09 23:29:55,144 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2017-02-09 23:29:55,146 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2017-02-09 23:29:55,146 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2017-02-09 23:29:55,146 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2017-02-09 23:29:55,262 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2017-02-09 23:29:55,263 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2017-02-09 23:29:55,287 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2017-02-09 23:29:55,287 INFO org.mortbay.log: jetty-6.1.26
2017-02-09 23:29:55,487 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2017-02-09 23:29:55,518 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:29:55,518 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2017-02-09 23:29:55,546 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2017-02-09 23:29:55,546 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2017-02-09 23:29:55,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2017-02-09 23:29:55,587 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2017-02-09 23:29:55,588 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2017-02-09 23:29:55,589 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2017 Feb 09 23:29:55
2017-02-09 23:29:55,590 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2017-02-09 23:29:55,590 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:29:55,591 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2017-02-09 23:29:55,592 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2017-02-09 23:29:55,597 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2017-02-09 23:29:55,597 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2017-02-09 23:29:55,597 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2017-02-09 23:29:55,597 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2017-02-09 23:29:55,597 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2017-02-09 23:29:55,597 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2017-02-09 23:29:55,597 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2017-02-09 23:29:55,597 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2017-02-09 23:29:55,603 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = leempan (auth:SIMPLE)
2017-02-09 23:29:55,603 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2017-02-09 23:29:55,603 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2017-02-09 23:29:55,603 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2017-02-09 23:29:55,604 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2017-02-09 23:29:55,743 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2017-02-09 23:29:55,743 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:29:55,743 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2017-02-09 23:29:55,743 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2017-02-09 23:29:55,743 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2017-02-09 23:29:55,744 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2017-02-09 23:29:55,744 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2017-02-09 23:29:55,744 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2017-02-09 23:29:55,749 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2017-02-09 23:29:55,750 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:29:55,750 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2017-02-09 23:29:55,750 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2017-02-09 23:29:55,751 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2017-02-09 23:29:55,751 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2017-02-09 23:29:55,751 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2017-02-09 23:29:55,753 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2017-02-09 23:29:55,753 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2017-02-09 23:29:55,753 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2017-02-09 23:29:55,755 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2017-02-09 23:29:55,755 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2017-02-09 23:29:55,756 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2017-02-09 23:29:55,757 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2017-02-09 23:29:55,757 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2017-02-09 23:29:55,757 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2017-02-09 23:29:55,771 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /opt/hadoop-2.7.3/tmp/dfs/name/in_use.lock acquired by nodename 72172@leempans-macbook-pro.local
2017-02-09 23:29:55,822 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /opt/hadoop-2.7.3/tmp/dfs/name/current
2017-02-09 23:29:55,864 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000042 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000042-0000000000000000042
2017-02-09 23:29:55,872 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000041, cpktTxId=0000000000000000041)
2017-02-09 23:29:55,908 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 6 INodes.
2017-02-09 23:29:56,037 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2017-02-09 23:29:56,037 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 41 from /opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000041
2017-02-09 23:29:56,037 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4504d271 expecting start txid #42
2017-02-09 23:29:56,037 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000042-0000000000000000042
2017-02-09 23:29:56,041 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000042-0000000000000000042' to transaction ID 42
2017-02-09 23:29:56,045 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000042-0000000000000000042 of size 1048576 edits # 1 loaded in 0 seconds
2017-02-09 23:29:56,052 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2017-02-09 23:29:56,054 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 43
2017-02-09 23:29:56,204 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2017-02-09 23:29:56,204 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 445 msecs
2017-02-09 23:29:56,505 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to ubuntu1:9000
2017-02-09 23:29:56,524 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2017-02-09 23:29:56,563 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2017-02-09 23:29:56,614 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2017-02-09 23:29:56,639 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:29:56,640 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2017-02-09 23:29:56,640 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 2 blocks to reach the threshold 0.9990 of total blocks 2.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2017-02-09 23:29:56,656 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:29:56,743 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2017-02-09 23:29:56,744 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2017-02-09 23:29:56,751 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: ubuntu1/192.168.1.11:9000
2017-02-09 23:29:56,752 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2017-02-09 23:29:56,773 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2017-02-09 23:29:58,345 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:29:58,347 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#0 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:29:59,472 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0) storage a08915f1-3c69-4b2c-ba27-0185b3d2a8f9
2017-02-09 23:29:59,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:29:59,473 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/192.168.1.11:50010
2017-02-09 23:29:59,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2017-02-09 23:29:59,551 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-49b10856-865b-4636-a955-077090905b46 for DN 192.168.1.11:50010
2017-02-09 23:29:59,589 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 1 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2017-02-09 23:29:59,589 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2017-02-09 23:29:59,590 INFO BlockStateChange: BLOCK* processReport: from storage DS-49b10856-865b-4636-a955-077090905b46 node DatanodeRegistration(192.168.1.11:50010, datanodeUuid=a08915f1-3c69-4b2c-ba27-0185b3d2a8f9, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-d5dcbf3f-288c-40c7-9701-58082a8990f6;nsid=1688747011;c=0), blocks: 2, hasStaleStorage: false, processing time: 7 msecs
2017-02-09 23:29:59,598 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 2
2017-02-09 23:29:59,598 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2017-02-09 23:29:59,598 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2017-02-09 23:29:59,598 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2017-02-09 23:29:59,598 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2017-02-09 23:29:59,598 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 9 msec
2017-02-09 23:30:03,370 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#1 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:08,374 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#2 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:13,378 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#3 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:18,383 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#4 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:19,641 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 2 has reached the threshold 0.9990 of total blocks 2. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2017-02-09 23:30:23,417 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#5 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:28,470 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:30:28,471 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#6 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:29,649 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 34 secs
2017-02-09 23:30:29,650 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2017-02-09 23:30:29,650 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2017-02-09 23:30:29,650 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 2 blocks
2017-02-09 23:30:33,474 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#7 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:38,554 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#8 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:43,667 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#9 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:48,781 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#10 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:53,886 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#11 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:30:58,926 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:30:58,927 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#12 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:03,930 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#13 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:04,530 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 192.168.1.11
2017-02-09 23:31:04,530 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2017-02-09 23:31:04,530 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 43
2017-02-09 23:31:04,531 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 3 
2017-02-09 23:31:04,533 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 4 
2017-02-09 23:31:04,534 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_inprogress_0000000000000000043 -> /opt/hadoop-2.7.3/tmp/dfs/name/current/edits_0000000000000000043-0000000000000000044
2017-02-09 23:31:04,534 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 45
2017-02-09 23:31:05,262 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2017-02-09 23:31:05,262 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000044 size 710 bytes.
2017-02-09 23:31:05,266 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 41
2017-02-09 23:31:05,267 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/opt/hadoop-2.7.3/tmp/dfs/name/current/fsimage_0000000000000000038, cpktTxId=0000000000000000038)
2017-02-09 23:31:09,032 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#14 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:14,073 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#15 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:19,077 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#16 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:24,081 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#17 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:29,113 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:31:29,113 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#18 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:34,118 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#19 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:39,203 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#20 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:44,207 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#21 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:49,212 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#22 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:54,227 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#23 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:31:59,273 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:31:59,273 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#24 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:04,366 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#25 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:09,476 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#26 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:14,626 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#27 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:19,703 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#28 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:24,723 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#29 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:29,749 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:32:29,750 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#30 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:34,756 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#31 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:39,776 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#32 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:44,874 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#33 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:49,978 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#34 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:32:55,108 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#35 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:00,229 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:33:00,229 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#36 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:05,329 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#37 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:10,484 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#38 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:15,498 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#39 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:20,504 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#40 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:25,528 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#41 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:30,605 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:33:30,605 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#42 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:35,613 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#43 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:40,616 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#44 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:45,622 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#45 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:50,645 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#46 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:33:55,739 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#47 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:00,886 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:34:00,886 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#48 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:05,967 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#49 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:11,081 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#50 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:16,084 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#51 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:21,104 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#52 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:25,288 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 10 
2017-02-09 23:34:25,335 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-49b10856-865b-4636-a955-077090905b46:NORMAL:192.168.1.11:50010|RBW]]} for /input/content.txt._COPYING_
2017-02-09 23:34:26,057 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741827_1003{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-49b10856-865b-4636-a955-077090905b46:NORMAL:192.168.1.11:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /input/content.txt._COPYING_
2017-02-09 23:34:26,057 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2017-02-09 23:34:26,074 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 192.168.1.11:50010 is added to blk_1073741827_1003{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-49b10856-865b-4636-a955-077090905b46:NORMAL:192.168.1.11:50010|RBW]]} size 36471086
2017-02-09 23:34:26,108 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#53 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:26,478 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /input/content.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1949634984_1
2017-02-09 23:34:31,148 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:34:31,148 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#54 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:36,245 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#55 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:41,262 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#56 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:46,320 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#57 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:47,395 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2017-02-09 23:34:47,396 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command rename is: 1
2017-02-09 23:34:47,396 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2017-02-09 23:34:47,396 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command rename is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command getfileinfo is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command rename is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command create is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command listStatus is: 1
2017-02-09 23:34:47,397 INFO org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: topN size for command * is: 1
2017-02-09 23:34:51,326 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#58 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:34:56,430 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#59 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:35:01,520 WARN org.apache.hadoop.security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user administrator: id: administrator: no such user
id: administrator: no such user

2017-02-09 23:35:01,520 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#60 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:35:06,616 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#61 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:35:11,641 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#62 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:35:16,747 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#63 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:35:21,856 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#64 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:35:26,861 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.server.protocol.DatanodeProtocol.versionRequest from 192.168.1.99:47728 Call#65 Retry#0: org.apache.hadoop.security.AccessControlException: Access denied for user administrator. Superuser privilege is required
2017-02-09 23:35:29,011 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2017-02-09 23:35:29,012 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at leempans-macbook-pro.local/192.168.1.11
************************************************************/
